#!/usr/bin/env python3
"""
æ”¹è¿›çš„è®­ç»ƒè„šæœ¬ï¼ŒåŒ…å«æ—©åœå’Œæ­£åˆ™åŒ–
"""
import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import os
import sys

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from data.balanced_dataset import get_balanced_dataloaders
from model.vit import VisionTransformer
from utils.training import Trainer

class EarlyStopping:
    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = float('inf')
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            if self.restore_best_weights:
                self.best_weights = model.state_dict().copy()
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights:
                model.load_state_dict(self.best_weights)
            return True
        return False

def train_with_early_stopping():
    print("ğŸ¯ è®­ç»ƒä¼˜åŒ–çš„ViTæ¨¡å‹ - é˜²æ­¢è¿‡æ‹Ÿåˆç‰ˆæœ¬")
    print("="*60)
    
    # ä¼˜åŒ–çš„è¶…å‚æ•°
    config = {
        'image_size': 32,
        'patch_size': 4, 
        'embed_dim': 192,  # å‡å°æ¨¡å‹å®¹é‡
        'num_layers': 8,   # é€‚ä¸­çš„æ·±åº¦
        'num_heads': 6,
        'batch_size': 128,
        'lr': 1e-3,
        'weight_decay': 0.05,  # å¢åŠ æ­£åˆ™åŒ–
        'dropout': 0.1,        # é€‚é‡dropout
        'num_epochs': 150,
        'patience': 10         # æ—©åœè€å¿ƒå€¼
    }
    
    # æ•°æ®åŠ è½½å™¨
    print("ğŸ“Š åŠ è½½æ•°æ®...")
    train_loader, val_loader = get_balanced_dataloaders(
        batch_size=config['batch_size'],
        image_size=config['image_size'],
        use_augmentation=True,      # ä½¿ç”¨æ•°æ®å¢å¼º
        use_balanced_sampling=True  # å¹³è¡¡é‡‡æ ·
    )
    
    # æ¨¡å‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  è®¾å¤‡: {device}")
    
    model = VisionTransformer(
        image_size=config['image_size'],
        patch_size=config['patch_size'],
        embed_dim=config['embed_dim'],
        num_layers=config['num_layers'],
        num_heads=config['num_heads'],
        dropout=config['dropout'],
        num_classes=10
    ).to(device)
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ”§ æ¨¡å‹å‚æ•°: {num_params:,}")
    
    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config['lr'],
        weight_decay=config['weight_decay']
    )
    
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, 
        T_max=config['num_epochs'],
        eta_min=config['lr'] * 0.01
    )
    
    # æ—©åœ
    early_stopping = EarlyStopping(
        patience=config['patience'], 
        min_delta=0.001,
        restore_best_weights=True
    )
    
    # è®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        criterion=nn.CrossEntropyLoss(),
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        log_dir='./logs_improved',
        checkpoint_dir='./checkpoints_improved'
    )
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    
    # è®­ç»ƒå¾ªç¯ï¼ˆå¸¦æ—©åœï¼‰
    best_val_acc = 0
    for epoch in range(config['num_epochs']):
        # è®­ç»ƒ
        train_loss, train_acc = trainer.train_epoch(epoch)
        
        # éªŒè¯
        val_loss, val_acc = trainer.validate(epoch)
        
        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step()
        
        # è®°å½•æ—¥å¿—
        trainer.writer.add_scalar('Train/Loss_Epoch', train_loss, epoch)
        trainer.writer.add_scalar('Train/Accuracy', train_acc, epoch)
        trainer.writer.add_scalar('Val/Loss_Epoch', val_loss, epoch)
        trainer.writer.add_scalar('Val/Accuracy', val_acc, epoch)
        trainer.writer.add_scalar('Learning_Rate', scheduler.get_last_lr()[0], epoch)
        
        # æ‰“å°ç»“æœ
        print(f'Epoch {epoch+1}/{config["num_epochs"]}:')
        print(f'  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')
        print(f'  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')
        print(f'  LR: {scheduler.get_last_lr()[0]:.2e}')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            trainer.save_checkpoint(epoch, is_best=True)
            print(f'  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.2f}%)')
        
        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss, model):
            print(f"\nâ¹ï¸  æ—©åœè§¦å‘! åœ¨ Epoch {epoch+1}")
            print(f"ğŸ¯ æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.2f}%")
            break
    
    trainer.writer.close()
    return best_val_acc

if __name__ == "__main__":
    final_acc = train_with_early_stopping()
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ! æœ€ç»ˆå‡†ç¡®ç‡: {final_acc:.2f}%")